# -*- coding: utf-8 -*-
"""Copy of Copy of FINAL COPY OF ML

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VtgXGjjYW1IRcO1Kaqt_xLYgCBE8fACL
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

import os 
import datetime
import numpy as np
import tensorflow as tf
from tensorflow import keras
import requests
import matplotlib.pyplot as plt

print ("Tensorflow version{}".format(tf.__version__))

os.listdir()

datafile_path = r'the_adventures_of_sherlock_holmes_1661-0.txt'

text = open(datafile_path, 'rb').read().decode(encoding='utf-8')
print('Book contains a total of {} characters'.format(len(text)))

text = text[1300:]

vocab = sorted(set(text))
print('{} unique characters'.format(len(vocab)))

char2idx = {u:i for i, u in enumerate(vocab)}
idx2char = np.array(vocab)
text_as_int = np.array([char2idx[c] for c in text])

print('{')
for char,_ in zip(char2idx , range(20)):
  print(' {:s}: {:d} ,'.format(repr(char), char2idx[char]))
print('  \n}')

seq_length = 100
examples_per_epoch = len(text)//(seq_length +1)

char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

for i in char_dataset.take(10):
  print(idx2char[i.numpy()])

sequences = char_dataset.batch(seq_length+1 , drop_remainder=True)

for item in sequences.take(10):
  print(repr(''.join(idx2char[item.numpy()])))

def split_input_target(chunk):
  input_text = chunk[:-1]
  target_text = chunk[1:]
  return input_text , target_text
dataset = sequences.map(split_input_target)

for input_example, target_example in dataset.take(1):
  print ('Input data:  ', repr(''.join(idx2char[input_example.numpy()])))
  print ('Target data: ', repr(''.join(idx2char[target_example.numpy()])))

for i, (input_idx , target_idx) in enumerate(zip(input_example[:5], target_example[:5])):
  print("Step {:d}".format(i))
  print("  input: {} ({:s})".format(input_idx , repr(idx2char[input_idx])))
  print("  expected output: {} ({:s})".format(target_idx, repr(idx2char[target_idx])))

BATCH_SIZE = 64
BUFFER_SIZE = 10000

dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE , drop_remainder = True)
print("Dataset Shape={}".format(dataset))

def build_model(vocab_size, embedding_dim , rnn_units,batch_size):
  model = tf.keras.Sequential([
  tf.keras.layers.Embedding(vocab_size , embedding_dim,
                            batch_input_shape=[batch_size,None]),
  tf.keras.layers.GRU(rnn_units,
                      return_sequences = True,
                      stateful = True,
                      recurrent_initializer='glorot_uniform'),
  tf.keras.layers.Dense(vocab_size)
                               
  ])
  return model

vocab_size = len(vocab)
embedding_dim = 256

rnn_units = 1024

model = build_model(
    vocab_size = len(vocab),
    embedding_dim=embedding_dim,
    rnn_units=rnn_units,
    batch_size=BATCH_SIZE)

model.summary()

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy' , metrics = 'accuracy' )



checkpoint_dir = r'data/training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir)

checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_prefix,
    save_weights_only=True
)

logdir = os.path.join("logs",datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1)

EPOCHS = 2
history = model.fit(dataset, epochs = EPOCHS , callbacks = [checkpoint_callback , tensorboard_callback])

history.history.keys()

x = y = plt.plot(history.history['loss'])
plt.ylabel("loss")
plt.title("LOSS GRAPH")
plt.show()

plt.plot(history.history['accuracy'])


plt.title("ACCURACY")
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

from tensorboard import notebook
notebook.list()

notebook.display(port=6006 , height = 1000)

tf.train.latest_checkpoint(checkpoint_dir)

model = build_model(vocab_size , embedding_dim, rnn_units , batch_size=1)
model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
model.build(tf.TensorShape([1,None]))

model.summary()

def generate_text(model , context_string,num_generate=100, temperature = 1.0):
  input_eval = [char2idx[s] for s in context_string ]
  input_eval = tf.expand_dims(input_eval , 0)

  text_generated = []
  model.reset_states()
  for i in range(num_generate):
    predictions = model(input_eval)
    predictions = tf.squeeze(predictions , 0)

    predictions = predictions / temperature
    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()
    input_eval = tf.expand_dims([predicted_id] , 0)
    text_generated.append(idx2char[predicted_id])
  return (context_string + ''.join(text_generated))

print(generate_text(model , context_string="I am going to ",num_generate=20 , temperature=0.1))

print(generate_text(model , context_string="How are  ",num_generate=30 , temperature=0.1))

print(generate_text(model , context_string="Who is ",num_generate=50 , temperature=0.3))

print(generate_text(model , context_string="I will",num_generate=30 , temperature=1.0))

print(generate_text(model , context_string="Can I get a ",num_generate=100 , temperature=0.1))

print(generate_text(model , context_string="Watson you are",num_generate=75 , temperature=0.2))